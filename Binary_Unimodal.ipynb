{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogpHzVbGY43x"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.transform import rotate\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras and TensorFlow imports\n",
        "import keras\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Concatenate, Flatten, GlobalAveragePooling2D, GlobalMaxPooling2D, Dropout, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.utils import Sequence\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Pre-trained models from Keras and TensorFlow\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50, EfficientNetV2S\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 GLOBAL VARIABLES:"
      ],
      "metadata": {
        "id": "174VqAzJUsGo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBwZCX_0Y8oX"
      },
      "outputs": [],
      "source": [
        "# General training hyperparameters\n",
        "EPOCHS = 20  # Number of times the model will iterate over the entire dataset\n",
        "BATCH_SIZE = 32  # Number of samples processed per batch during training\n",
        "\n",
        "# Image-related constants\n",
        "IMG_HEIGHT = 224  # Input image height (in pixels)\n",
        "IMG_WIDTH = 224  # Input image width (in pixels)\n",
        "MODELS_INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)  # Model input shape (height, width, channels)\n",
        "\n",
        "# Data-related constants\n",
        "VALIDATION_DATASET_SIZE = 0.2  # Fraction of data to be used for validation (20%)\n",
        "FILENAME_COLUMN = 'id'  # Column name for unique file identifiers\n",
        "TARGET_VARIABLE = 'T1'  # Column name for the target variable (classification label)\n",
        "DATA_LOCATION = \"\"  # Path to dataset storage\n",
        "\n",
        "# Optimization constants\n",
        "ADAM_COEFFICIENT = 0.001  # Learning rate for the Adam optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 DATA LOADING:"
      ],
      "metadata": {
        "id": "-cEDTF_GUyZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv48kUc5ZsVI"
      },
      "outputs": [],
      "source": [
        "original_data = pd.read_csv(DATA_LOCATION + 'binary_dataset.csv')\n",
        "original_data[FILENAME_COLUMN] = original_data[FILENAME_COLUMN].apply(lambda x: DATA_LOCATION + 'images/' + x + '.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 DEALING WITH IMBALANCED DATA"
      ],
      "metadata": {
        "id": "FPhy_cMPU5dR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze4T7JDDXzcL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def return_datasets(data_df):\n",
        "    \"\"\"\n",
        "    Balances the dataset by oversampling the minority class and then splits it into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        data_df (pd.DataFrame): The input DataFrame containing features and the target column 'T1'.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Two DataFrames (X_train and X_test) with features and target variable included.\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine the imbalance in class distribution (number of samples to add)\n",
        "    gapnum = abs(data_df[data_df.T1 == 1].shape[0] - data_df[data_df.T1 != 1].shape[0])\n",
        "\n",
        "    # Randomly sample additional instances from the minority class (T1 == 1)\n",
        "    temp_df = data_df.iloc[np.random.choice(np.where(data_df[TARGET_VARIABLE] == 1)[0], size=gapnum)]\n",
        "\n",
        "    # Add the oversampled data back into the original dataset\n",
        "    data_df = pd.concat([data_df, temp_df], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset to ensure randomness\n",
        "    data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    # Separate the target variable ('T1') from the features\n",
        "    target_df = data_df.T1\n",
        "    data_df.drop(columns=[TARGET_VARIABLE], inplace=True)\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data_df, target_df, test_size=VALIDATION_DATASET_SIZE, random_state=42\n",
        "    )\n",
        "\n",
        "    # Reattach the target variable to the training and test sets\n",
        "    X_train[TARGET_VARIABLE] = y_train\n",
        "    X_test[TARGET_VARIABLE] = y_test\n",
        "\n",
        "    # Return the processed training and validation datasets\n",
        "    return X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
        "\n",
        "# Prepare the dataset by applying the function\n",
        "train_df, val_df = return_datasets(original_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 DATAGENERATORKERAS CLASS:"
      ],
      "metadata": {
        "id": "LDccL0VOU88s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ_CWL1ZdGjI"
      },
      "outputs": [],
      "source": [
        "class DataGeneratorKeras(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Data Generator for loading and preprocessing image datasets.\n",
        "\n",
        "    This generator loads images from a dataset, applies optional augmentation and preprocessing,\n",
        "    and returns batches of images along with their corresponding labels.\n",
        "\n",
        "    Attributes:\n",
        "        dataset (pd.DataFrame): DataFrame containing image file paths and target labels.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        image_shape (tuple): Shape of the input images (height, width, channels).\n",
        "        filename_column (str): Column name containing image file paths.\n",
        "        target_column (list): Column(s) containing target labels.\n",
        "        augmentation (bool): Whether to apply data augmentation.\n",
        "        preprocessing_fn (callable): Function to apply preprocessing to images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, augmentation=False, preprocessing_fn=None,\n",
        "                 filename_column='id', target_column=['T1'], image_shape=(224, 224, 3), batch_size=16):\n",
        "        \"\"\"\n",
        "        Initializes the data generator.\n",
        "\n",
        "        Args:\n",
        "            dataset (pd.DataFrame): DataFrame containing image paths and target labels.\n",
        "            augmentation (bool, optional): Whether to apply data augmentation. Defaults to False.\n",
        "            preprocessing_fn (callable, optional): Function for additional image preprocessing. Defaults to None.\n",
        "            filename_column (str, optional): Column name with image file paths. Defaults to 'id'.\n",
        "            target_column (list, optional): List of column names containing labels. Defaults to ['T1'].\n",
        "            image_shape (tuple, optional): Shape of the input images. Defaults to (224, 224, 3).\n",
        "            batch_size (int, optional): Number of samples per batch. Defaults to 16.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.image_shape = image_shape\n",
        "        self.filename_column = filename_column\n",
        "        self.target_column = target_column\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Computes the number of batches per epoch.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of batches per epoch.\n",
        "        \"\"\"\n",
        "        return len(self.dataset) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Shuffles the dataset at the end of each epoch to improve training variability.\n",
        "        \"\"\"\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates a batch of data.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "            tuple: ((images, images), labels), where images are preprocessed input data,\n",
        "                   and labels are the corresponding target values.\n",
        "        \"\"\"\n",
        "        images = np.empty((self.batch_size, *self.image_shape), dtype=np.float32)\n",
        "        labels = np.empty((self.batch_size, len(self.target_column)), dtype=np.float32)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            img_index = idx * self.batch_size + i\n",
        "            image_path = self.dataset.iloc[img_index][self.filename_column]\n",
        "            image = img_to_array(load_img(image_path, target_size=self.image_shape[:2]))\n",
        "\n",
        "            # Apply optional augmentation\n",
        "            if self.augmentation:\n",
        "                image = rotate(image, np.random.uniform(-30, 30), preserve_range=True)\n",
        "                if np.random.choice([True, False]):  # Random horizontal flip\n",
        "                    image = np.flip(image, axis=1)\n",
        "\n",
        "            # Apply optional preprocessing function\n",
        "            if self.preprocessing_fn:\n",
        "                image = self.preprocessing_fn(image)\n",
        "\n",
        "            images[i] = image\n",
        "            labels[i] = self.dataset.iloc[img_index][self.target_column].values.astype(np.float32)\n",
        "\n",
        "        return images, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 GENERATE THE DATAGENERATOR TO BE USED IN THE MODEL TRAINING:"
      ],
      "metadata": {
        "id": "b4oZBZgKVBNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2w7vdjRdWPB"
      },
      "outputs": [],
      "source": [
        "train_datagen = DataGeneratorKeras(dataset = train_df, augmentation = True, preprocessing_fn = preprocess_input, batch_size = 32)\n",
        "valid_datagen = DataGeneratorKeras(dataset = val_df, augmentation = False, preprocessing_fn = preprocess_input, batch_size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 MODEL ARCHITECTURE:"
      ],
      "metadata": {
        "id": "oThen8Y9VJQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKtOtrsVdfMp"
      },
      "outputs": [],
      "source": [
        "# Define the input\n",
        "inputs = Input(shape=MODELS_INPUT_SHAPE)\n",
        "\n",
        "# Load ResNet50 with the input tensor\n",
        "#include_top = False because otherwise it would include the layers for the 1000 classes.\n",
        "#base_model = VGG19(weights='imagenet', input_tensor=inputs)\n",
        "#base_model = InceptionV3(weights='imagenet', input_tensor=inputs)\n",
        "base_model = ResNet50(weights='imagenet', input_tensor=inputs)\n",
        "#base_model = EfficientNetV2S(weights='imagenet', input_tensor=inputs)\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add global average pooling layer\n",
        "x = Flatten(name='flatten2')(base_model.output)\n",
        "#x = GlobalAveragePooling2D()(base_model.output)\n",
        "for units in [256, 128, 64, 32, 16]:\n",
        "    x = Dense(units=units, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer = Adam(ADAM_COEFFICIENT), loss = 'binary_crossentropy', metrics = ['accuracy', Precision(), Recall()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 MODEL TRAINING:"
      ],
      "metadata": {
        "id": "4T4BKNnMVNDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgWp66whSbKK"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    x = train_datagen,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = valid_datagen\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}