{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import rotate, AffineTransform, warp, resize\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Keras and TensorFlow applications\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.efficientnet_v2 import EfficientNetV2S\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# Keras models and layers\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Input, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.utils import Sequence\n",
        "\n",
        "# Keras image preprocessing\n",
        "from keras.preprocessing.image import load_img, img_to_array"
      ],
      "metadata": {
        "id": "LbMjAUzxAOzH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 GLOBAL VARIABLES:"
      ],
      "metadata": {
        "id": "B4eEyXGfeY3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image dimensions and input shape for models\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "MODELS_INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "# Prefix for identifying layers in the Places-365 model for Early Fusion\n",
        "PLACES_PREFIX = 'places'\n",
        "\n",
        "# Data-related constants\n",
        "VALIDATION_DATASET_SIZE = 0.2  # Proportion of dataset used for validation\n",
        "DATA_LOCATION = ''\n",
        "DATA_IMAGES_LOCATION = DATA_LOCATION + 'images/'\n",
        "TARGET_VARIABLE_NAME = 'T1'\n",
        "IMAGE_FILENAME_COLUMN = 'id'\n",
        "\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_LAYERS = [256, 128, 64, 32, 16]  # Dense layer sizes for feature fusion\n",
        "NUM_CLASSES = 2  # Number of output classes for classification\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LOSS_FUNCTION = 'binary_crossentropy'"
      ],
      "metadata": {
        "id": "8gD4Dh8DAO9G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 DATA LOADING:"
      ],
      "metadata": {
        "id": "vkT3Op15BG88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_data = pd.read_csv(DATA_LOCATION + 'binary_dataset.csv')\n",
        "#id contains the filename without the image format. Just include .jpg to obtain it\n",
        "original_data['id'] = original_data[IMAGE_FILENAME_COLUMN].apply(lambda x: DATA_LOCATION + 'images/' + x + '.jpg')"
      ],
      "metadata": {
        "id": "a4sfyMkHBGTi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_datasets(data_df):\n",
        "    # Create empty DataFrames with the same columns as the input data\n",
        "    train_df = pd.DataFrame(columns=data_df.columns)\n",
        "    val_df = pd.DataFrame(columns=data_df.columns)\n",
        "\n",
        "    # Generate an array of indices and split them into training and validation sets\n",
        "    train_inds, val_inds = train_test_split(\n",
        "        np.array(list(range(data_df.shape[0]))),  # Array of row indices for data_df\n",
        "        test_size=VALIDATION_DATASET_SIZE,        # Fraction of data to be used for validation\n",
        "        random_state=42                           # Ensures reproducibility of the split\n",
        "    )\n",
        "\n",
        "    # Assign rows corresponding to the training indices to train_df and reset the index\n",
        "    train_df = data_df.iloc[train_inds, :].reset_index(drop=True)\n",
        "\n",
        "    # Assign rows corresponding to the validation indices to val_df and reset the index\n",
        "    val_df = data_df.iloc[val_inds, :].reset_index(drop=True)\n",
        "\n",
        "    # Return the training and validation DataFrames\n",
        "    return train_df, val_df\n",
        "\n",
        "train_df, val_df = return_datasets(original_data)"
      ],
      "metadata": {
        "id": "D3oofUIABQ2b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 DEALING WITH IMBALANCED DATA"
      ],
      "metadata": {
        "id": "Hwh-YzMsBcFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = train_df[TARGET_VARIABLE_NAME].value_counts()\n",
        "\n",
        "# Find the minimum count of any value in the column\n",
        "min_count = value_counts.min()\n",
        "\n",
        "# Create a balanced DataFrame\n",
        "balanced_df = pd.concat([\n",
        "    train_df[train_df[TARGET_VARIABLE_NAME] == value].sample(min_count, random_state=42)\n",
        "        for value in value_counts.index\n",
        "    ])\n",
        "\n",
        "#Shuffle the dataset to mix the labels\n",
        "balanced_df = balanced_df.sample(n=len(balanced_df))\n",
        "train_df = balanced_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ceYk-oLsyQgB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 DATAGENERATORKERAS CLASS:"
      ],
      "metadata": {
        "id": "4618-k6pBv-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGeneratorKeras(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Data Generator for loading and preprocessing image datasets.\n",
        "\n",
        "    This generator loads images from a dataset, applies optional augmentation and preprocessing,\n",
        "    and returns batches of images along with their corresponding labels.\n",
        "\n",
        "    Attributes:\n",
        "        dataset (pd.DataFrame): DataFrame containing image file paths and target labels.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        image_shape (tuple): Shape of the input images (height, width, channels).\n",
        "        filename_column (str): Column name containing image file paths.\n",
        "        target_column (list): Column(s) containing target labels.\n",
        "        augmentation (bool): Whether to apply data augmentation.\n",
        "        preprocessing_fn (callable): Function to apply preprocessing to images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, augmentation=False, preprocessing_fn=None,\n",
        "                 filename_column='id', target_column=['T1'], image_shape=(224, 224, 3), batch_size=16):\n",
        "        \"\"\"\n",
        "        Initializes the data generator.\n",
        "\n",
        "        Args:\n",
        "            dataset (pd.DataFrame): DataFrame containing image paths and target labels.\n",
        "            augmentation (bool, optional): Whether to apply data augmentation. Defaults to False.\n",
        "            preprocessing_fn (callable, optional): Function for additional image preprocessing. Defaults to None.\n",
        "            filename_column (str, optional): Column name with image file paths. Defaults to 'id'.\n",
        "            target_column (list, optional): List of column names containing labels. Defaults to ['T1'].\n",
        "            image_shape (tuple, optional): Shape of the input images. Defaults to (224, 224, 3).\n",
        "            batch_size (int, optional): Number of samples per batch. Defaults to 16.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.image_shape = image_shape\n",
        "        self.filename_column = filename_column\n",
        "        self.target_column = target_column\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Computes the number of batches per epoch.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of batches per epoch.\n",
        "        \"\"\"\n",
        "        return len(self.dataset) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Shuffles the dataset at the end of each epoch to improve training variability.\n",
        "        \"\"\"\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates a batch of data.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "            tuple: ((images, images), labels), where images are preprocessed input data,\n",
        "                   and labels are the corresponding target values.\n",
        "        \"\"\"\n",
        "        images = np.empty((self.batch_size, *self.image_shape), dtype=np.float32)\n",
        "        labels = np.empty((self.batch_size, len(self.target_column)), dtype=np.float32)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            img_index = idx * self.batch_size + i\n",
        "            image_path = self.dataset.iloc[img_index][self.filename_column]\n",
        "            image = img_to_array(load_img(image_path, target_size=self.image_shape[:2]))\n",
        "\n",
        "            # Apply optional augmentation\n",
        "            if self.augmentation:\n",
        "                image = rotate(image, np.random.uniform(-30, 30), preserve_range=True)\n",
        "                if np.random.choice([True, False]):  # Random horizontal flip\n",
        "                    image = np.flip(image, axis=1)\n",
        "\n",
        "            # Apply optional preprocessing function\n",
        "            if self.preprocessing_fn:\n",
        "                image = self.preprocessing_fn(image)\n",
        "\n",
        "            images[i] = image\n",
        "            labels[i] = self.dataset.iloc[img_index][self.target_column].values.astype(np.float32)\n",
        "\n",
        "        return (images, images), labels\n"
      ],
      "metadata": {
        "id": "jC0iiuJ4zMNW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4 GENERATE THE DATAGENERATOR TO BE USED IN THE MODEL TRAINING:"
      ],
      "metadata": {
        "id": "PDeKmBXbKf6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = DataGeneratorKeras(dataset = train_df, augmentation = True, preprocessing_fn = preprocess_input, batch_size = 32)\n",
        "valid_datagen = DataGeneratorKeras(dataset = val_df, augmentation = False, preprocessing_fn = preprocess_input, batch_size = 32)"
      ],
      "metadata": {
        "id": "8Q0slVsW0rrN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 PLACES365 CLASS:"
      ],
      "metadata": {
        "id": "NQK6vYsNBUcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, MaxPooling2D, Conv2D\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import get_file\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/GKalliatakis/Keras-VGG16-places365/releases/download/v1.0/vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "L2_PARAMETER = 0.0002\n",
        "\n",
        "\n",
        "def conv_block(x, filters, convs, prefix):\n",
        "    \"\"\"\n",
        "    Creates a convolutional block with a given number of convolutional layers.\n",
        "\n",
        "    Args:\n",
        "        x (tensor): Input tensor.\n",
        "        filters (int): Number of filters for each convolutional layer.\n",
        "        convs (int): Number of convolutional layers in the block.\n",
        "        prefix (str): Prefix for layer names.\n",
        "\n",
        "    Returns:\n",
        "        tensor: Output tensor after applying convolutions and max pooling.\n",
        "    \"\"\"\n",
        "    for i in range(convs):\n",
        "        x = Conv2D(filters, kernel_size=3, padding='same',\n",
        "                   kernel_regularizer=l2(L2_PARAMETER), activation='relu',\n",
        "                   name=f'{prefix}_conv{i+1}')(x)\n",
        "    return MaxPooling2D(pool_size=(2, 2), name=f'{prefix}_pool')(x)\n",
        "\n",
        "def VGG16_Places365(weights='places', input_shape=None, prefix=''):\n",
        "\n",
        "    classes = 365\n",
        "\n",
        "    \"\"\"\n",
        "    Builds the VGG16 model for Places365 classification.\n",
        "\n",
        "    Args:\n",
        "        include_top (bool): Whether to include the fully connected layers.\n",
        "        weights (str): Type of pre-trained weights to use ('places' for Places365).\n",
        "        input_shape (tuple): Shape of the input images.\n",
        "        prefix (str): Prefix for layer names.\n",
        "\n",
        "    Returns:\n",
        "        Model: Keras model instance of VGG16-Places365.\n",
        "    \"\"\"\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "\n",
        "    # Define the convolutional blocks\n",
        "    x = conv_block(input_tensor, filters=64, convs=2, prefix=f'{prefix}block1')\n",
        "    x = conv_block(x, filters=128, convs=2, prefix=f'{prefix}block2')\n",
        "    x = conv_block(x, filters=256, convs=3, prefix=f'{prefix}block3')\n",
        "    x = conv_block(x, filters=512, convs=3, prefix=f'{prefix}block4')\n",
        "    x = conv_block(x, filters=512, convs=3, prefix=f'{prefix}block5')\n",
        "\n",
        "    # Fully connected layers\n",
        "    x = Flatten(name=f'{prefix}flatten')(x)\n",
        "    x = Dense(4096, activation='relu', name=f'{prefix}fc1')(x)\n",
        "    x = Dropout(0.5, name=f'{prefix}drop_fc1')(x)\n",
        "    x = Dense(4096, activation='relu', name=f'{prefix}fc2')(x)\n",
        "    x = Dropout(0.5, name=f'{prefix}drop_fc2')(x)\n",
        "    x = Dense(classes, activation='softmax', name=f'{prefix}predictions')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(input_tensor, x, name='vgg16-places365')\n",
        "\n",
        "    # Load weights only if requested\n",
        "    if weights == 'places':\n",
        "        weights_path = get_file('vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                WEIGHTS_PATH, cache_subdir='models')\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SDjX8Sn201kQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 EARLY-FUSION MODEL GENERATION:"
      ],
      "metadata": {
        "id": "Qz_xhDa-EZj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_places_365_model():\n",
        "    places365_model = VGG16_Places365(weights='places', prefix=PLACES_PREFIX, input_shape = (MODELS_INPUT_SHAPE))\n",
        "    places365_model.trainable = False\n",
        "    return places365_model\n",
        "\n",
        "\n",
        "def create_earlyfusion_model(cnn_model, model_layer):\n",
        "    cnn_model. trainable = False\n",
        "    places365_model = create_places_365_model()\n",
        "    model_365_features = places365_model.get_layer(f'{PLACES_PREFIX}fc2').output\n",
        "    cnn_model_features = cnn_model.get_layer(model_layer).output\n",
        "\n",
        "    x = Concatenate()([model_365_features, cnn_model_features])\n",
        "    for units in [256, 128, 64, 32, 16]:\n",
        "        x = Dense(units=units, activation='relu')(x)\n",
        "\n",
        "    x = Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=[places365_model.input, cnn_model.input], outputs=x, name='early-fusion')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ViACKdHW04oe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = Input(shape=MODELS_INPUT_SHAPE)\n",
        "\n",
        "models = {\n",
        "    #'vgg19_binary.keras': (VGG19(weights='imagenet', input_tensor=input_tensor), 'fc2')\n",
        "    'inceptionv3_binary.keras': (InceptionV3(weights='imagenet', input_tensor=input_tensor), 'avg_pool')\n",
        "    #'resnet50_binary.keras': (ResNet50(weights='imagenet', input_tensor=input_tensor), 'avg_pool'),\n",
        "    #'efficientnetv2s_binary.keras': (EfficientNetV2S(weights='imagenet', input_tensor=input_tensor), 'avg_pool')\n",
        "}\n",
        "\n",
        "model_to_use = 'inceptionv3_binary.keras'\n",
        "\n",
        "early_fusion_model = create_earlyfusion_model(models[model_to_use][0], models[model_to_use][1])\n",
        "\n",
        "\n",
        "early_fusion_model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "        metrics=['accuracy', Precision(), Recall()]\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "id": "eqbX-E3BCFH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d672038-adaa-4534-bbe0-657f8a566fe9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m96112376/96112376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "Downloading data from https://github.com/GKalliatakis/Keras-VGG16-places365/releases/download/v1.0/vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5\n",
            "\u001b[1m543085444/543085444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.1 MODEL TRAINING:"
      ],
      "metadata": {
        "id": "ZYMxXR7pEk29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = early_fusion_model.fit(\n",
        "    x = train_datagen,\n",
        "    epochs = EPOCHS,\n",
        "    validation_data = valid_datagen\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP6Z7-_CEYA8",
        "outputId": "ef1c2b2d-bfdd-43cb-82f1-6300df481fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor_313', ['keras_tensor']]\n",
            "Received: inputs=('Tensor(shape=(None, 224, 224, 3))', 'Tensor(shape=(None, 224, 224, 3))')\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/50\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:41\u001b[0m 29s/step - accuracy: 0.5484 - loss: 1.0859 - precision: 0.5693 - recall: 0.6563"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9JYLkE636Yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}