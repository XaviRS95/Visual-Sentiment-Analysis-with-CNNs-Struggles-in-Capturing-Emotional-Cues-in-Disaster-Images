{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf7x0RbGXHNX",
        "outputId": "9c75e3f2-283c-4a1f-8120-e09546386bbb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "uq63OkK3WPpE"
      },
      "outputs": [],
      "source": [
        "# Standard Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "from skimage.transform import rotate, AffineTransform, warp\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.metrics import Precision, Recall\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import Sequence\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# Data Balancing and Splitting\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ICBwPMdkkLen"
      },
      "outputs": [],
      "source": [
        "# Image dimensions and input shape for models\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "MODELS_INPUT_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "# Data-related constants\n",
        "VALIDATION_DATASET_SIZE = 0.2  # Proportion of dataset used for validation\n",
        "DATA_LOCATION = ''\n",
        "DATA_IMAGES_LOCATION = DATA_LOCATION + 'images/'\n",
        "TARGET_VARIABLE_NAME = 'T1'\n",
        "IMAGE_FILENAME_COLUMN = 'id'\n",
        "NUM_CLASSES = 0\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "HIDDEN_LAYERS = [256, 128, 64, 32, 16]  # Dense layer sizes for feature fusion\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LOSS_FUNCTION = 'binary_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(DATA_LOCATION + 'T2_powerlabel_dataset.csv')"
      ],
      "metadata": {
        "id": "A6kOlQMDWuUf"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "XWWeYSI4o79Y"
      },
      "outputs": [],
      "source": [
        "def return_datasets(data_df):\n",
        "    # Create empty DataFrames with the same columns as the input data\n",
        "    train_df = pd.DataFrame(columns=data_df.columns)\n",
        "    val_df = pd.DataFrame(columns=data_df.columns)\n",
        "\n",
        "    # Generate an array of indices and split them into training and validation sets\n",
        "    train_inds, val_inds = train_test_split(\n",
        "        np.array(list(range(data_df.shape[0]))),  # Array of row indices for data_df\n",
        "        test_size=VALIDATION_DATASET_SIZE,        # Fraction of data to be used for validation\n",
        "        random_state=42                           # Ensures reproducibility of the split\n",
        "    )\n",
        "\n",
        "    # Assign rows corresponding to the training indices to train_df and reset the index\n",
        "    train_df = data_df.iloc[train_inds, :].reset_index(drop=True)\n",
        "\n",
        "    # Assign rows corresponding to the validation indices to val_df and reset the index\n",
        "    val_df = data_df.iloc[val_inds, :].reset_index(drop=True)\n",
        "\n",
        "    # Return the training and validation DataFrames\n",
        "    return train_df, val_df\n",
        "\n",
        "train_df, val_df = return_datasets(data)\n",
        "\n",
        "#Oversampling:\n",
        "\n",
        "powercount = {}\n",
        "powerlabels = np.unique(train_df['powerlabel'])\n",
        "for p in powerlabels:\n",
        "    powercount[p] = np.count_nonzero(train_df['powerlabel']==p)\n",
        "\n",
        "maxcount = np.max(list(powercount.values()))\n",
        "for p in powerlabels:\n",
        "    gapnum = maxcount - powercount[p]\n",
        "    temp_df = train_df.iloc[np.random.choice(np.where(train_df['powerlabel']==p)[0],size=gapnum)]\n",
        "    train_df = pd.concat([train_df, temp_df], ignore_index=True)\n",
        "\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "train_df.drop(columns=['powerlabel'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "vZjGK7PZo-Zn"
      },
      "outputs": [],
      "source": [
        "class_names = list(data.columns)[1:-1]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGeneratorKeras(Sequence):\n",
        "    \"\"\"\n",
        "    Keras Data Generator for loading and preprocessing image datasets.\n",
        "\n",
        "    This generator loads images from a dataset, applies optional augmentation and preprocessing,\n",
        "    and returns batches of images along with their corresponding labels.\n",
        "\n",
        "    Attributes:\n",
        "        dataset (pd.DataFrame): DataFrame containing image file paths and target labels.\n",
        "        batch_size (int): Number of samples per batch.\n",
        "        image_shape (tuple): Shape of the input images (height, width, channels).\n",
        "        filename_column (str): Column name containing image file paths.\n",
        "        target_column (list): Column(s) containing target labels.\n",
        "        augmentation (bool): Whether to apply data augmentation.\n",
        "        preprocessing_fn (callable): Function to apply preprocessing to images.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, augmentation=False, preprocessing_fn=None,\n",
        "                 filename_column='id', target_column=['T1'], image_shape=(224, 224, 3), batch_size=16):\n",
        "        \"\"\"\n",
        "        Initializes the data generator.\n",
        "\n",
        "        Args:\n",
        "            dataset (pd.DataFrame): DataFrame containing image paths and target labels.\n",
        "            augmentation (bool, optional): Whether to apply data augmentation. Defaults to False.\n",
        "            preprocessing_fn (callable, optional): Function for additional image preprocessing. Defaults to None.\n",
        "            filename_column (str, optional): Column name with image file paths. Defaults to 'id'.\n",
        "            target_column (list, optional): List of column names containing labels. Defaults to ['T1'].\n",
        "            image_shape (tuple, optional): Shape of the input images. Defaults to (224, 224, 3).\n",
        "            batch_size (int, optional): Number of samples per batch. Defaults to 16.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.image_shape = image_shape\n",
        "        self.filename_column = filename_column\n",
        "        self.target_column = target_column\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Computes the number of batches per epoch.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of batches per epoch.\n",
        "        \"\"\"\n",
        "        return len(self.dataset) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Shuffles the dataset at the end of each epoch to improve training variability.\n",
        "        \"\"\"\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates a batch of data.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the batch.\n",
        "\n",
        "        Returns:\n",
        "            tuple: ((images, images), labels), where images are preprocessed input data,\n",
        "                   and labels are the corresponding target values.\n",
        "        \"\"\"\n",
        "        images = np.empty((self.batch_size, *self.image_shape), dtype=np.float32)\n",
        "        labels = np.empty((self.batch_size, NUM_CLASSES), dtype=np.float32)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            img_index = idx * self.batch_size + i\n",
        "            image_path = self.dataset.iloc[img_index][self.filename_column]\n",
        "            image = img_to_array(load_img(image_path, target_size=self.image_shape[:2]))\n",
        "\n",
        "            # Apply optional augmentation\n",
        "            if self.augmentation:\n",
        "                image = rotate(image, np.random.uniform(-30, 30), preserve_range=True)\n",
        "                if np.random.choice([True, False]):  # Random horizontal flip\n",
        "                    image = np.flip(image, axis=1)\n",
        "\n",
        "            # Apply optional preprocessing function\n",
        "            if self.preprocessing_fn:\n",
        "                image = self.preprocessing_fn(image)\n",
        "\n",
        "            images[i] = image\n",
        "            labels[i] = self.dataset.iloc[idx*self.batch_size+i][class_names].values.astype(np.float32)\n",
        "\n",
        "        return images, labels"
      ],
      "metadata": {
        "id": "gHxfVvP1fwSS"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "woD6orSbpz6z"
      },
      "outputs": [],
      "source": [
        "train_datagen = DataGeneratorKeras(dataset = train_df, augmentation = True, preprocessing_fn = preprocess_input, batch_size = BATCH_SIZE)\n",
        "valid_datagen = DataGeneratorKeras(dataset = val_df, augmentation = False, preprocessing_fn = preprocess_input, batch_size = BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input\n",
        "inputs = Input(shape=MODELS_INPUT_SHAPE)\n",
        "\n",
        "#base_model = VGG19(weights='imagenet', input_tensor=inputs)\n",
        "#base_model = InceptionV3(weights='imagenet', input_tensor=inputs)\n",
        "#base_model = ResNet50(weights='imagenet', input_tensor=inputs)\n",
        "#base_model = EfficientNetV2S(weights='imagenet', input_tensor=inputs)\n",
        "base_model = DenseNet121(input_tensor=inputs)\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add global average pooling layer\n",
        "x = Flatten(name='flatten2')(base_model.output)\n",
        "#x = GlobalAveragePooling2D()(base_model.output)\n",
        "for units in [256, 128, 64, 32, 16]:\n",
        "    x = Dense(units=units, activation='relu')(x)\n",
        "outputs = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer = Adam(LEARNING_RATE), loss = LOSS_FUNCTION, metrics = ['accuracy', Precision(), Recall()])"
      ],
      "metadata": {
        "id": "m_ioTT3eN1yq"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJXFgzeaqWg6",
        "outputId": "0431dbb7-7a5e-4b6a-bfd1-a57a056d89b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m 12/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49:10\u001b[0m 10s/step - accuracy: 0.3900 - loss: 0.6919 - precision_2: 0.4650 - recall_2: 0.6031"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_datagen, epochs = EPOCHS, verbose = 1, validation_data = valid_datagen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wrXyr8Sw9HO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}